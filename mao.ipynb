{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright: Wentao Shi, 2021\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Copyright: Wentao Shi, 2021\n",
    "import torch\n",
    "import re\n",
    "import yaml\n",
    "import selfies as sf\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# Copyright: Wentao Shi, 2021\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "import selfies as sf\n",
    "\n",
    "\n",
    "\n",
    "# suppress rdkit error\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:176: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:176: SyntaxWarning: invalid escape sequence '\\['\n",
      "/tmp/ipykernel_2481174/2881812017.py:176: SyntaxWarning: invalid escape sequence '\\['\n",
      "  regex = '(\\[[^\\[\\]]{1,6}\\])'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dataloader_gen(dataset_dir, percentage, which_vocab, vocab_path,\n",
    "                   batch_size, PADDING_IDX, shuffle, drop_last=True):\n",
    "    \"\"\"\n",
    "    Genrate the dataloader for training\n",
    "    \"\"\"\n",
    "    if which_vocab == \"selfies\":\n",
    "        vocab = SELFIEVocab(vocab_path)\n",
    "    elif which_vocab == \"regex\":\n",
    "        vocab = RegExVocab(vocab_path)\n",
    "    elif which_vocab == \"char\":\n",
    "        vocab = CharVocab(vocab_path)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong vacab name for configuration which_vocab!\")\n",
    "\n",
    "    dataset = SMILESDataset(dataset_dir, percentage, vocab)\n",
    "\n",
    "    def pad_collate(batch):\n",
    "        \"\"\"\n",
    "        Put the sequences of different lengths in a minibatch by paddding.\n",
    "        \"\"\"\n",
    "        lengths = [len(x) for x in batch]\n",
    "\n",
    "        # embedding layer takes long tensors\n",
    "        batch = [torch.tensor(x, dtype=torch.long) for x in batch]\n",
    "\n",
    "        x_padded = pad_sequence(\n",
    "            batch, \n",
    "            batch_first=True,\n",
    "            padding_value=PADDING_IDX\n",
    "        )\n",
    "\n",
    "        return x_padded, lengths\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last, \n",
    "        collate_fn=pad_collate\n",
    "    )\n",
    "\n",
    "    return dataloader, len(dataset)\n",
    "\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_file, percentage, vocab):\n",
    "        \"\"\"\n",
    "        smiles_file: path to the .smi file containing SMILES.\n",
    "        percantage: percentage of the dataset to use.\n",
    "        \"\"\"\n",
    "        super(SMILESDataset, self).__init__()\n",
    "        assert(0 < percentage <= 1)\n",
    "\n",
    "        self.percentage = percentage\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # load eaqual portion of data from each tranche\n",
    "        self.data = self.read_smiles_file(smiles_file)\n",
    "        print(\"total number of SMILES loaded: \", len(self.data))\n",
    "\n",
    "        # convert the smiles to selfies\n",
    "        if self.vocab.name == \"selfies\":\n",
    "            self.data = [sf.encoder(x)\n",
    "                         for x in self.data if sf.encoder(x) is not None]\n",
    "            print(\"total number of valid SELFIES: \", len(self.data))\n",
    "\n",
    "    def read_smiles_file(self, path):\n",
    "        # need to exclude first line which is not SMILES\n",
    "        with open(path, \"r\") as f:\n",
    "            smiles = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "        num_data = len(smiles)\n",
    "\n",
    "        return smiles[0:int(num_data * self.percentage)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mol = self.data[index]\n",
    "\n",
    "        # convert the data into integer tokens\n",
    "        mol = self.vocab.tokenize_smiles(mol)\n",
    "\n",
    "        return mol\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CharVocab:\n",
    "    def __init__(self, vocab_path):\n",
    "        self.name = \"char\"\n",
    "\n",
    "        # load the pre-computed vocabulary\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = yaml.full_load(f)\n",
    "\n",
    "        # a dictionary to map integer back to SMILES\n",
    "        # tokens for sampling\n",
    "        self.int2tocken = {}\n",
    "        for token, num in self.vocab.items():\n",
    "            self.int2tocken[num] = token\n",
    "\n",
    "        # a hashset of tokens for O(1) lookup\n",
    "        self.tokens = self.vocab.keys()\n",
    "\n",
    "    def tokenize_smiles(self, smiles):\n",
    "        \"\"\"\n",
    "        Takes a SMILES string and returns a list of tokens.\n",
    "        Atoms with 2 characters are treated as one token. The \n",
    "        logic references this code piece:\n",
    "        https://github.com/topazape/LSTM_Chem/blob/master/lstm_chem/utils/smiles_tokenizer2.py\n",
    "        \"\"\"\n",
    "        n = len(smiles)\n",
    "        tokenized = ['<sos>']\n",
    "        i = 0\n",
    "\n",
    "        # process all characters except the last one\n",
    "        while (i < n - 1):\n",
    "            # procoss tokens with length 2 first\n",
    "            c2 = smiles[i:i + 2]\n",
    "            if c2 in self.tokens:\n",
    "                tokenized.append(c2)\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            # tokens with length 2\n",
    "            c1 = smiles[i]\n",
    "            if c1 in self.tokens:\n",
    "                tokenized.append(c1)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            raise ValueError(\n",
    "                \"Unrecognized charater in SMILES: {}, {}\".format(c1, c2))\n",
    "\n",
    "        # process last character if there is any\n",
    "        if i == n:\n",
    "            pass\n",
    "        elif i == n - 1 and smiles[i] in self.tokens:\n",
    "            tokenized.append(smiles[i])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unrecognized charater in SMILES: {}\".format(smiles[i]))\n",
    "\n",
    "        tokenized.append('<eos>')\n",
    "\n",
    "        tokenized = [self.vocab[token] for token in tokenized]\n",
    "        return tokenized\n",
    "\n",
    "    def combine_list(self, smiles):\n",
    "        return \"\".join(smiles)\n",
    "\n",
    "\n",
    "class RegExVocab:\n",
    "    def __init__(self, vocab_path):\n",
    "        self.name = \"regex\"\n",
    "\n",
    "        # load the pre-computed vocabulary\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = yaml.full_load(f)\n",
    "\n",
    "        # a dictionary to map integer back to SMILES\n",
    "        # tokens for sampling\n",
    "        self.int2tocken = {}\n",
    "        for token, num in self.vocab.items():\n",
    "            if token == \"R\":\n",
    "                self.int2tocken[num] = \"Br\"\n",
    "            elif token == \"L\":\n",
    "                self.int2tocken[num] = \"Cl\"\n",
    "            else:\n",
    "                self.int2tocken[num] = token\n",
    "\n",
    "    def tokenize_smiles(self, smiles):\n",
    "        \"\"\"Takes a SMILES string and returns a list of tokens.\n",
    "        This will swap 'Cl' and 'Br' to 'L' and 'R' and treat\n",
    "        '[xx]' as one token.\"\"\"\n",
    "        regex = '(\\[[^\\[\\]]{1,6}\\])'\n",
    "        smiles = self.replace_halogen(smiles)\n",
    "        char_list = re.split(regex, smiles)\n",
    "\n",
    "        tokenized = ['<sos>']\n",
    "\n",
    "        for char in char_list:\n",
    "            if char.startswith('['):\n",
    "                tokenized.append(char)\n",
    "            else:\n",
    "                chars = [unit for unit in char]\n",
    "                [tokenized.append(unit) for unit in chars]\n",
    "        tokenized.append('<eos>')\n",
    "\n",
    "        # convert tokens to integer tokens\n",
    "        tokenized = [self.vocab[token] for token in tokenized]\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def replace_halogen(self, string):\n",
    "        \"\"\"Regex to replace Br and Cl with single letters\"\"\"\n",
    "        br = re.compile('Br')\n",
    "        cl = re.compile('Cl')\n",
    "        string = br.sub('R', string)\n",
    "        string = cl.sub('L', string)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def combine_list(self, smiles):\n",
    "        return \"\".join(smiles)\n",
    "\n",
    "\n",
    "class SELFIEVocab:\n",
    "    def __init__(self, vocab_path):\n",
    "        self.name = \"selfies\"\n",
    "\n",
    "        # load the pre-computed vocabulary\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = yaml.full_load(f)\n",
    "\n",
    "        self.int2tocken = {value: key for key, value in self.vocab.items()}\n",
    "\n",
    "    def tokenize_smiles(self, mol):\n",
    "        \"\"\"convert the smiles to selfies, then return \n",
    "        integer tokens.\"\"\"\n",
    "        ints = [self.vocab['<sos>']]\n",
    "\n",
    "        #encoded_selfies = sf.encoder(smiles)\n",
    "        selfies_list = list(sf.split_selfies(mol))\n",
    "        for token in selfies_list:\n",
    "            ints.append(self.vocab[token])\n",
    "\n",
    "        ints.append(self.vocab['<eos>'])\n",
    "\n",
    "        return ints\n",
    "\n",
    "    def combine_list(self, selfies):\n",
    "        return \"\".join(selfies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_config):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=rnn_config['num_embeddings'],\n",
    "            embedding_dim=rnn_config['embedding_dim'],\n",
    "            padding_idx=rnn_config['num_embeddings'] - 1\n",
    "        )\n",
    "\n",
    "        if rnn_config['rnn_type'] == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=rnn_config['input_size'],\n",
    "                hidden_size=rnn_config['hidden_size'],\n",
    "                num_layers=rnn_config['num_layers'],\n",
    "                batch_first=True,\n",
    "                dropout=rnn_config['dropout']\n",
    "            )\n",
    "        elif rnn_config['rnn_type'] == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=rnn_config['input_size'],\n",
    "                hidden_size=rnn_config['hidden_size'],\n",
    "                num_layers=rnn_config['num_layers'],\n",
    "                batch_first=True,\n",
    "                dropout=rnn_config['dropout']\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"rnn_type should be either 'LSTM' or 'GRU'.\"\n",
    "            )\n",
    "\n",
    "        # output does not include <sos> and <pad>, so\n",
    "        # decrease the num_embeddings by 2\n",
    "        self.linear = nn.Linear(\n",
    "            rnn_config['hidden_size'], rnn_config['num_embeddings'] - 2\n",
    "        )\n",
    "\n",
    "    def forward(self, data, lengths):\n",
    "        embeddings = self.embedding_layer(data)\n",
    "\n",
    "        # pack the padded input\n",
    "        # the lengths are decreased by 1 because we don't\n",
    "        # use <eos> for input and we don't need <sos> for\n",
    "        # output during traning.\n",
    "        embeddings = pack_padded_sequence(\n",
    "            input=embeddings, \n",
    "            lengths=lengths, \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # recurrent network, discard (h_n, c_n) in output.\n",
    "        # Tearcher-forcing is used here, so we directly feed\n",
    "        # the whole sequence to model.\n",
    "        embeddings, _ = self.rnn(embeddings)\n",
    "\n",
    "        # linear layer to generate input of softmax\n",
    "        embeddings = self.linear(embeddings.data)\n",
    "\n",
    "        # return the packed representation for backpropagation,\n",
    "        # the targets will also be packed.\n",
    "        return embeddings\n",
    "\n",
    "    def sample(self, batch_size, vocab, device, max_length=140):\n",
    "        \"\"\"Use this function if device is GPU\"\"\"\n",
    "        # get integer of \"start of sequence\"\n",
    "        start_int = vocab.vocab['<sos>']\n",
    "\n",
    "        # create a tensor of shape [batch_size, seq_step=1]\n",
    "        sos = torch.ones(\n",
    "            [batch_size, 1], \n",
    "            dtype=torch.long, \n",
    "            device=device\n",
    "        )\n",
    "        sos = sos * start_int\n",
    "\n",
    "        # sample first output\n",
    "        output = []\n",
    "        x = self.embedding_layer(sos)\n",
    "        x, hidden = self.rnn(x)\n",
    "        x = self.linear(x)\n",
    "        x = softmax(x, dim=-1)\n",
    "        x = torch.multinomial(x.squeeze(), 1)\n",
    "        output.append(x)\n",
    "\n",
    "        # a tensor to indicate if the <eos> token is found\n",
    "        # for all data in the mini-batch\n",
    "        finish = torch.zeros(batch_size, dtype=torch.bool).to(device)\n",
    "\n",
    "        # sample until every sequence in the mini-batch\n",
    "        # has <eos> token\n",
    "        for _ in range(max_length):\n",
    "            # forward rnn\n",
    "            x = self.embedding_layer(x)\n",
    "            x, hidden = self.rnn(x, hidden)\n",
    "            x = self.linear(x)\n",
    "            x = softmax(x, dim=-1)\n",
    "            \n",
    "            # sample\n",
    "            x = torch.multinomial(x.squeeze(), 1)\n",
    "            output.append(x)\n",
    "\n",
    "            # terminate if <eos> is found for every data\n",
    "            eos_sampled = (x == vocab.vocab['<eos>']).data\n",
    "            finish = torch.logical_or(finish, eos_sampled.squeeze())\n",
    "            if torch.all(finish):\n",
    "                return torch.cat(output, -1)\n",
    "\n",
    "        return torch.cat(output, -1)\n",
    "\n",
    "    def sample_cpu(self, vocab):\n",
    "        \"\"\"Use this function if device is CPU\"\"\"\n",
    "        output = []\n",
    "\n",
    "        # get integer of \"start of sequence\"\n",
    "        start_int = vocab.vocab['<sos>']\n",
    "\n",
    "        # create a tensor of shape [batch_size=1, seq_step=1]\n",
    "        sos = torch.tensor(\n",
    "            start_int, \n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(dim=0\n",
    "        ).unsqueeze(dim=0)\n",
    "\n",
    "        # sample first output\n",
    "        x = self.embedding_layer(sos)\n",
    "        x, hidden = self.rnn(x)\n",
    "        x = self.linear(x)\n",
    "        x = softmax(x, dim=-1)\n",
    "        x = torch.multinomial(x.squeeze(), 1)\n",
    "        output.append(x.item())\n",
    "\n",
    "        # use first output to iteratively sample until <eos> occurs\n",
    "        while output[-1] != vocab.vocab['<eos>']:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "            x = self.embedding_layer(x)\n",
    "            x, hidden = self.rnn(x, hidden)\n",
    "            x = self.linear(x)\n",
    "            x = softmax(x, dim=-1)\n",
    "            x = torch.multinomial(x.squeeze(), 1)\n",
    "            output.append(x.item())\n",
    "\n",
    "        # convert integers to tokens\n",
    "        output = [vocab.int2tocken[x] for x in output]\n",
    "\n",
    "        # popout <eos>\n",
    "        output.pop()\n",
    "\n",
    "        # convert to a single string\n",
    "        output = vocab.combine_list(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def make_vocab(config):\n",
    "    # load vocab\n",
    "    which_vocab = config[\"which_vocab\"]\n",
    "    vocab_path = config[\"vocab_path\"]\n",
    "\n",
    "    if which_vocab == \"selfies\":\n",
    "        return SELFIEVocab(vocab_path)\n",
    "    elif which_vocab == \"regex\":\n",
    "        return RegExVocab(vocab_path)\n",
    "    elif which_vocab == \"char\":\n",
    "        return CharVocab(vocab_path)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Wrong vacab name for configuration which_vocab!\"\n",
    "        )\n",
    "\n",
    "\n",
    "def sample(model, vocab, batch_size):\n",
    "    \"\"\"Sample a batch of SMILES from current model.\"\"\"\n",
    "    model.eval()\n",
    "    # sample\n",
    "    sampled_ints = model.sample(\n",
    "        batch_size=batch_size,\n",
    "        vocab=vocab,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # convert integers back to SMILES\n",
    "    molecules = []\n",
    "    sampled_ints = sampled_ints.tolist()\n",
    "    for ints in sampled_ints:\n",
    "        molecule = []\n",
    "        for x in ints:\n",
    "            if vocab.int2tocken[x] == '<eos>':\n",
    "                break\n",
    "            else:\n",
    "                molecule.append(vocab.int2tocken[x])\n",
    "        molecules.append(\"\".join(molecule))\n",
    "\n",
    "    # convert SELFIES back to SMILES\n",
    "    if vocab.name == 'selfies':\n",
    "        molecules = [sf.decoder(x) for x in molecules]\n",
    "\n",
    "    return molecules\n",
    "\n",
    "\n",
    "def compute_valid_rate(molecules):\n",
    "    \"\"\"compute the percentage of valid SMILES given\n",
    "    a list SMILES strings\"\"\"\n",
    "    num_valid, num_invalid = 0, 0\n",
    "    for mol in molecules:\n",
    "        mol = Chem.MolFromSmiles(mol)\n",
    "        if mol is None:\n",
    "            num_invalid += 1\n",
    "        else:\n",
    "            num_valid += 1\n",
    "\n",
    "    return num_valid, num_invalid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "# detect cpu or gpu\n",
    "device = torch.device(\n",
    "    'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print('device: ', device)\n",
    "\n",
    "config_dir = \"./train.yaml\"\n",
    "with open(config_dir, 'r') as f:\n",
    "    config = yaml.full_load(f)\n",
    "\n",
    "# directory for results\n",
    "out_dir = config['out_dir']\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "trained_model_dir = out_dir + 'trained_model.pt'\n",
    "\n",
    "# save the configuration file for future reference\n",
    "with open(out_dir + 'config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "number of workers to load data:  32\n",
      "which vocabulary to use:  selfies\n",
      "total number of SMILES loaded:  538247\n",
      "total number of valid SELFIES:  538247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training data\n",
    "dataset_dir = config['dataset_dir']\n",
    "which_vocab = config['which_vocab']\n",
    "vocab_path = config['vocab_path']\n",
    "percentage = config['percentage']\n",
    "\n",
    "# create dataloader\n",
    "batch_size = config['batch_size']\n",
    "shuffle = config['shuffle']\n",
    "PADDING_IDX = config['rnn_config']['num_embeddings'] - 1\n",
    "num_workers = os.cpu_count()\n",
    "print('number of workers to load data: ', num_workers)\n",
    "print('which vocabulary to use: ', which_vocab)\n",
    "dataloader, train_size = dataloader_gen(\n",
    "    dataset_dir, percentage, which_vocab,\n",
    "    vocab_path, batch_size, PADDING_IDX,\n",
    "    shuffle, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding_layer): Embedding(48, 256, padding_idx=47)\n",
       "  (rnn): GRU(256, 512, num_layers=3, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=46, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model and training configuration\u001b[39;00m\n\u001b[1;32m      3\u001b[0m rnn_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_config\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m RNN(rnn_config)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/anshul/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/.conda/envs/anshul/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/anshul/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/anshul/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"########################################\")\n",
    "# model and training configuration\n",
    "rnn_config = config['rnn_config']\n",
    "model = RNN(rnn_config).to(device)\n",
    "learning_rate = config['learning_rate']\n",
    "weight_decay = config['weight_decay']\n",
    "\n",
    "# Making reduction=\"sum\" makes huge difference\n",
    "# in valid rate of sampled molecules.\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# create optimizer\n",
    "if config['which_optimizer'] == \"adam\":\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate,\n",
    "        weight_decay=weight_decay, amsgrad=True\n",
    "    )\n",
    "elif config['which_opti0mizer'] == \"sgd\":\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=learning_rate,\n",
    "        weight_decay=weight_decay, momentum=0.9\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Wrong optimizer! Select between 'adam' and 'sgd'.\"\n",
    "    )\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min',\n",
    "    factor=0.5, patience=5,\n",
    "    cooldown=10, min_lr=0.0001,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# vocabulary object used by the sample() function\n",
    "vocab = make_vocab(config)\n",
    "\n",
    "# train and validation, the results are saved.\n",
    "train_losses = []\n",
    "best_valid_rate = 0\n",
    "num_epoch = config['num_epoch']\n",
    "\n",
    "print('begin training...')\n",
    "for epoch in range(1, 1 + num_epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, lengths in tqdm(dataloader):\n",
    "        # the lengths are decreased by 1 because we don't\n",
    "        # use <eos> for input and we don't need <sos> for\n",
    "        # output during traning.\n",
    "        lengths = [length - 1 for length in lengths]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        preds = model(data, lengths)\n",
    "\n",
    "        # The <sos> token is removed before packing, because\n",
    "        # we don't need <sos> of output during training.\n",
    "        # the image_captioning project uses the same method\n",
    "        # which directly feeds the packed sequences to\n",
    "        # the loss function:\n",
    "        # https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/train.py\n",
    "        targets = pack_padded_sequence(\n",
    "            data[:, 1:],\n",
    "            lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        ).data\n",
    "\n",
    "        loss = loss_function(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate loss over mini-batches\n",
    "        train_loss += loss.item()  # * data.size()[0]\n",
    "\n",
    "    train_losses.append(train_loss / train_size)\n",
    "\n",
    "    print('epoch {}, train loss: {}.'.format(epoch, train_losses[-1]))\n",
    "\n",
    "    scheduler.step(train_losses[-1])\n",
    "\n",
    "    # sample 1024 SMILES each epoch\n",
    "    sampled_molecules = sample(model, vocab, batch_size=1024)\n",
    "\n",
    "    # print the valid rate each epoch\n",
    "    num_valid, num_invalid = compute_valid_rate(sampled_molecules)\n",
    "    valid_rate = num_valid / (num_valid + num_invalid)\n",
    "\n",
    "    print('valid rate: {}'.format(valid_rate))\n",
    "\n",
    "    # update the saved model upon best validation loss\n",
    "    if valid_rate >= best_valid_rate:\n",
    "        best_valid_rate = valid_rate\n",
    "        print('model saved at epoch {}'.format(epoch))\n",
    "        torch.save(model.state_dict(), trained_model_dir)\n",
    "\n",
    "# save train and validation losses\n",
    "with open(out_dir + 'loss.yaml', 'w') as f:\n",
    "    yaml.dump(train_losses, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
