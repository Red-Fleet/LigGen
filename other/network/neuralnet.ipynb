{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import os\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from os import path as p\n",
    "import torchtext\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "VOCAB_SIZE = 32\n",
    "DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocab(smiles_dir, max_size=100):\n",
    "    '''max_size should be greater than 4'''\n",
    "    all_toks_dict = defaultdict(int)\n",
    "\n",
    "    for f_name in os.listdir(smiles_dir):\n",
    "        with open(os.path.join(smiles_dir, f_name)) as f:\n",
    "            for line in f:\n",
    "                if 'smile' in line: continue\n",
    "                for tok in atomwise_tokenizer(line.split()[0]):\n",
    "                    all_toks_dict[tok] += 1\n",
    "\n",
    "    \n",
    "    x = sorted([(e[0], e[1]) for e in all_toks_dict.items()], key=lambda e:-e[1])[:VOCAB_SIZE-4]\n",
    "    ordered_dict = OrderedDict({e[0]: e[1] for e in x})\n",
    "    print(ordered_dict)\n",
    "    special_tokens = [PAD_TOKEN, UNK_TOKEN, START_TOKEN, END_TOKEN]\n",
    "    vocab = torchtext.vocab.vocab(ordered_dict=ordered_dict, specials=special_tokens)\n",
    "    vocab.set_default_index(vocab[UNK_TOKEN])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset creation\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles_dir, vocab, max_len=None): # start and end tokens are added\n",
    "        '''\n",
    "        if vocab is None : getVocab() is used\n",
    "        if max_len is None: take max_len from dataset\n",
    "        if max_len == 'avg': take average length from dataset\n",
    "        '''\n",
    "        self.vocab = vocab\n",
    "\n",
    "        smiles_files = os.listdir(smiles_dir)\n",
    "        # reading smiles from files\n",
    "        tokens = []\n",
    "        for file_name in smiles_files:\n",
    "            with open(p.join(smiles_dir, file_name)) as f:\n",
    "                for l in f:\n",
    "                    if \"smile\" not in l:\n",
    "                        tokens.append(atomwise_tokenizer(l.split()[0]))\n",
    "        \n",
    "        if max_len is None: max_len = max([len(sen) for sen in tokens])\n",
    "        if max_len == 'avg': max_len = int(sum([len(sen) for sen in tokens])/len(tokens))\n",
    "\n",
    "        # stripping\n",
    "        tokens = [sen[: max_len-2] for sen in tokens]\n",
    "\n",
    "        # adding start and end tokens\n",
    "        tokens = [[START_TOKEN] + sen + [END_TOKEN] for sen in tokens]\n",
    "\n",
    "        # padding\n",
    "        tokens = [sen + [PAD_TOKEN]*(max_len-len(sen)) for sen in tokens]\n",
    "\n",
    "        # addention mask false at <pad> tokem, true at non pad token\n",
    "        self.pad_masks = torch.tensor([[PAD_TOKEN==tok for tok in sen] for sen in tokens])\n",
    "\n",
    "        # converting to index\n",
    "        self.data = torch.tensor([self.vocab(sen) for sen in tokens], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'idx': self.data[idx], 'pad_mask': self.pad_masks[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'c': 23438656, 'C': 17638257, '(': 12944012, ')': 12944012, '1': 8613576, 'O': 6589370, '=': 4349916, 'N': 3933209, '2': 3724200, 'n': 2283405, '3': 1063448, '[C@H]': 977539, '[C@@H]': 856458, 'F': 806720, '/': 582009, 'S': 528853, 'Cl': 482588, '[nH]': 259200, '4': 245628, 's': 225655, 'o': 217697, '\\\\': 137672, '[C@]': 128017, '#': 125602, '.': 125217, '[O-]': 116643, '[C@@]': 114773, '[N+]': 102860, 'Br': 93663, '5': 57606, 'P': 45820, '-': 40535, '[n+]': 33506, '6': 16166, 'I': 13157, '[Na+]': 13109, '[Br-]': 6907, '[S+]': 6557, '7': 6496, '[Cl-]': 5098})\n"
     ]
    }
   ],
   "source": [
    "vocab = getVocab('smiles', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmilesDataset('smiles', vocab, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999\n",
      "199999\n",
      "299999\n",
      "399999\n",
      "499999\n",
      "599999\n",
      "699999\n",
      "799999\n",
      "899999\n",
      "999999\n",
      "1099999\n",
      "1199999\n",
      "1299999\n",
      "1399999\n",
      "1499999\n",
      "1599999\n",
      "1699999\n",
      "1799999\n",
      "1899999\n"
     ]
    }
   ],
   "source": [
    "classes = [0]*100\n",
    "\n",
    "all_x = []\n",
    "for i in range(0, len(dataset)):\n",
    "    if (i+1)%100000 == 0: print(i)\n",
    "    all_x.append(dataset[i]['idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = torch.concat(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    classes[i] = (all_x == i).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35764)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941411"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "split_seed = 42\n",
    "train_ds, valid_ds = random_split(dataset, [int(len(dataset)*split_ratio), len(dataset)-int(len(dataset)*split_ratio)], generator=torch.Generator().manual_seed(split_seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x.transpose(0, 1)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        x = x.transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward,  dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multihead = nn.MultiheadAttention(embed_dim=d_model, \n",
    "                                               num_heads=nhead, \n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, attn_mask, pad_mask):\n",
    "        x = x + self.dropout1(self.multihead(x, x, x, key_padding_mask=pad_mask, attn_mask=attn_mask, need_weights=False)[0])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout3(self.linear2(self.dropout2(self.linear1(x))))\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, dim_feedforward, num_layers=1, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, nhead, dim_feedforward) for i in range(num_layers)])\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def getDevice(self):\n",
    "        ''' return device of model\n",
    "        '''\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        if device.index is None:\n",
    "            return device.type\n",
    "        else:\n",
    "            return device.type + \":\" + str(device.index) \n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz:int) -> torch.Tensor:\n",
    "        r\"\"\"Generate a square causal mask for the sequence.\n",
    "\n",
    "        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\n",
    "        device_ordinal = {-1: cpu, 0..inf=>gpu}\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.triu(\n",
    "            torch.full((sz, sz), float('-inf'), dtype=torch.float, device=self.getDevice()),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        ''' x = batch * seq_len\n",
    "            pad_mask = batch*seq_len\n",
    "        '''\n",
    "        seq_len = x.shape[1]\n",
    "        attn_mask = self.generate_square_subsequent_mask(seq_len)\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, attn_mask=attn_mask, pad_mask=pad_mask)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    def generateSmiles(self, batch_size, vocab, max_len=100):\n",
    "        x = torch.full((batch_size, 1), vocab[START_TOKEN]).to(self.getDevice())\n",
    "        \n",
    "        for i in range(1, max_len+1):\n",
    "            out = self.forward(x)[:, -1]\n",
    "            out = f.softmax(out, dim=1)\n",
    "            out = torch.multinomial(out, 1)\n",
    "            x = torch.cat((x, out), dim=1)\n",
    "        \n",
    "        x = x.detach().cpu().tolist()\n",
    "        # converting idx to smiles\n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            sentance = vocab.lookup_tokens(x[i])\n",
    "            \n",
    "            new_sentance = [] # removing special chars\n",
    "            for i in range(1, len(sentance)):\n",
    "                e = sentance[i]\n",
    "                if e==END_TOKEN: break\n",
    "                if e==START_TOKEN or e==PAD_TOKEN or e==UNK_TOKEN:\n",
    "                    new_sentance = ['invalid']\n",
    "                    break\n",
    "                \n",
    "                new_sentance.append(e)\n",
    "            \n",
    "            results.append(new_sentance)\n",
    "        \n",
    "        return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size=VOCAB_SIZE, \n",
    "        d_model=256, \n",
    "        nhead=16, \n",
    "        dim_feedforward=128, \n",
    "        num_layers=3, \n",
    "        dropout=0.1, \n",
    "        max_len=100)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size)\n",
    "val_loader = DataLoader(valid_ds, batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "train_result = []\n",
    "val_result = []\n",
    "epoch  = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1(C2[C@@H]([C@@H]([C@H](O2)CO)O)O)c2c(nc1)c(ncn2)N(C)C\n",
      "C1[C@@H](C[n+]2c(C1)sc(n2)NC(=O)Cn1cc(nn1)Nc1ccc(cc1Cl)Cl)O.[Br-]\n",
      "CCN(C)c1cc(cc(Nc2ncnc3[nH]c(cc23)C#N)c1)C(=O)N1CCC[C@H]1CO\n",
      "O=C1N(C(=O)NC(=O)C1C)C(=O)c1cc2nc[nH]c2cc1\n",
      "N1(C[C@H]([C@@H](C1)O)N)C(=O)CCSCC\n",
      "Fc1ccc(NC(=O)c2nc3c(cc2C)cccc3)cc1\n",
      "C12=CC[C@H]3[C@]4([C@H]2[C@](CC(C4)([C@H](CCCC(=O)C[C@@H]1C(C)C)C)C5)C)(CC[C@@H](C(=O)O)C)C)C\n",
      "O=C([C@@H]1CN(C[C@H]1c1ccc(s1)c1cccc(c1)C(F)(F)F)CC(=O)Nc1ccc(nc1)C(F)(F)F)c1ccc(cc1)C(F)(F)F\n",
      "C1(=C(NC(=C(C1c1cc(c(c(c1)OC)OC)OC)C(=O)NCC)C)C)C(=O)c1ccncc1\n",
      "Clc1c(Cl)ccc(NC(=O)COc2ccc(S(=O)(=O)N3CCOCC3)cc2)c1\n",
      "Br.Brc1c(OC[C@H](CCCCN2CCN(CC2)C)O)cccc1.Cl\n",
      "c1c(c(c2c(n1)nc(cc2C)SCC1CCN(CC1)C)C)C\n",
      "n1(cnc(c1)c1sccc1)c1cc2c(cc1)ccs2\n",
      "[Br-].c1c(cc2c(c1)c(c[nH]2)CCC#C)NC(=O)c1ccc(cc1)Cl\n",
      "CC(C)(O)Cc1[nH]c2cncc(N(C)C(C)C)c2n1\n",
      "C[n+]1coc(n1)/C=N/OC\n",
      "c1(c(cc2c(c1)c1c([nH]2)cnc(c1)Oc1ccc(cc1)O)O)OC\n",
      "C1(NC(=O)C(N(C1=O)c1ccccc1)C)(NC(=O)C1CC1)C1CC\n",
      "Clc1c(/C=C/2\\N(C(=O)c3oc(cc3)c3c(Cl)cccc3)Cc2cccc2)cccc1\n",
      "FC(F)(F)c1cc(N2CCN(CC2)c2ccc(NC(=O)c3n(nc(c3)C)C)cc2)ccc1OC\n",
      "CC(C)(C)/C=C/c1cc2nc(/C=C/4\\NC(=O)c5ccccc5-c3c2cc1)c1c(C)cccc1\n",
      "c1(nc(c[nH]1)CNC(=O)c1cccc(c1)Cl)O[C@@H]1CC[C@H](CC1)Oc1ccc2c(n1)[nH]c(=O)[nH]2\n",
      "O=C(N(C(C(=O)OCC)CC(=O)NCc1c2c([nH]c1)cccc2)C(C)C)c1ccccc1\n",
      "c1c2c3c(ccc1)nc(s3)N/N=C/1\\C(=C/c3c[nH]c4c3cccc4)/C(=C1)(CCC)CC2\n",
      "c1(c(cc2c(c1)c(ncn2)Nc1cc(c(c(c1)F)F)OC)Sc1nc2c([nH]1)cc(cc2)F)F\n",
      "o1nc(nc1CN1CCC(C1)C(=O)O)c1ccc(cc1)Cl\n",
      "[C@@H]1(CN(CCC1)CCCC)C(=O)N(C)C\n",
      "S(=O)(=O)(CCC1=C2CC(C3C(C1(=O)NCC[C@H](OC)CCCCC)CCC2)c1ccc(OC)cc1)c1ccc(OC)cc1\n",
      "O(c1c(OC)cc(cc1OC)/C=N/NC(=O)Nc1ccccc1)C\n",
      "N1(C(=O)[C@@H](NC(=O)CCCN)Cc2ccc(O)cc2)[C@H](C(=O)O)CCC1\n",
      "S1C2(N(C(=O)C1)c1ncccc1)CC(=O)N(C2=O)c1cc(cc(c1)c1ccccc1)C\n",
      "[nH]1c2c(=O)n(c3c1cncc3)CCOC2Oc1ccc([C@@H]2NC(=O)C)cc1\n",
      "O=C1N(C(=O)c2c1cccc2)CCC(=O)NCc1ccc(OC)cc1\n",
      "c1(C(=O)Nc2c(cc(cc2)F)F)sc2c(c1)cccc2\n",
      "N([C@H](C(=O)Nc1nc(c(n(c1C)C(=N)N)C)SCc1ccccc1)CC(C)C)C\n",
      "C1(=O)[C@H](NC(=O)[C@H]2N1C[C@@H](C2)N1CCN(CC1)C(=O)Nc1ccccc1)CC(=O)O\n",
      "s1c(/C=C/C(=O)N2CCCC2)ccc1\n",
      "Clc1cc(n2c(=O)c3c(n(c(=O)n(c3=O)CCCCC)n(c2=O)C)nc2c1Cl)C\n",
      "C12(C(=O)NCCCN1)c1c(C(=O)N[C@H](CC(=O)N[C@@H](C(=O)NCCc3cc(ccc3)O)C)CCCCNC(=N)N)cccc1OCc1ccccc1)cccc2\n",
      "N(C(=O)Nc1ccc(cc1)S(=O)(=O)Cc1ccc(cc1)O)(c1ccccc1)C\n",
      "Cn1cc(nc1C(=O)N1[C@H]2C[C@@]2(CS1(=O)=O)C(=O)CC2)c1nnn(c1)c1ccc(F)cc1\n",
      "O(c1nc(nc2c1cccc2)Nc1ccc(cc1)C(=O)OC)c1ccc(cc1)C\n",
      "Oc1n2-n3c(C(S(=O)(=O)Nc4ccccc4)Cc2nc2c1cccc2)cccc3\n",
      "c12c(oc(=O)c1CCC1N1CCN(CC1)C(=O)OCC)cccc2\n",
      "S=c1n(c(=O)c2c([nH]1)ccc(C(F)(F)F)c2)Cc1ccccc1\n",
      "C123c4c5O[C@@H]([C@H]1/C=C/C[C@@H]([C@@H]2C/C=C/[C@@H]([C@@H]([C@]([C@@H]([C@@H](C(=O)[C@@H]([C@@H]([C@@H]([C@H](/C=C/C=O)C)O)(C)C)O)C)O)O)O)O)C)/CC)C\n",
      "O=C(NC[C@H](N([C@@H](C(=O)N)C(=O)N([C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)NCC(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H]\n",
      "Fc1ccc(CC(=O)N2CCC(CC2)Cc2occc2)cc1.Cl\n",
      "OC1(CCN(CC1)Cc1ccccc1)Cc1nnc(s1)c1ccns1\n",
      "s1c(C(=O)Nc2cnc(Nc3c(F)cccc3)cc2)ccc1\n",
      "CC(C)Oc1nc(nc2c1c(ccc2)n1nnc(N)C(=O)Nc2ccc(NC(=O)N3CCOC(Cl)C3)cc2)c1c([N+](=O)[O-])cc(cc1)[N+](=O)[O-]\n",
      "Brc1c(S(=O)(=O)N2CCN(c3ncccn3)CC2)cccc1\n",
      "S(=O)(=O)(c1c(c2ccc(nc2)F)cc2nc(n[nH]2)ccc(c1)OC)NCC1CCCCC1\n",
      "S(=O)(=O)(Nc1cc(ccc1)C(=O)OC)c1ccc(cc1)C\n",
      "invalid\n",
      "S1(=O)(=O)Cc2c(C(=O)C1)cccc2\n",
      "c1c(ccc(c1)OCCCCCN1CCN(CC1)C)C(NC(=O)C1CCCCO1)C\n",
      "C1N(CCN(C1)c1nc2ccc(cc2[nH]1)c1nnn(c1)C)c1ccccc1\n",
      "O=C(N(c1ccccc1)c1ccccc1)c1ccc(NCCCCN2CCCC2)cc1\n",
      "O=C(N(N[C@@H](C(=O)NCCNC(=O)OCc1ccccc1)C)Cc1ccc(c2ccccc2)cc1)C\n",
      "S(=O)(=O)(N1CCC(CC1)C(=O)Nc1nc(n(c1)C)C)c1sccc1\n",
      "O=S(=O)(c1c(cc2c(c1)oc(=O)c(c2CO)C(=O)N(C2CC2)CCCO)NC(=O)c1[nH]c(c(c1)Cl)Cl)F\n",
      "C12C(C(CCC1CC1(CC2CN(C1)c1ccncc1)C(C)C)(O)C)c1cc(cc(c1)Cl)Cl\n",
      "Oc1nnc(OC)[nH]1\n",
      "c1ccc(cc1)N1C(=O)c2c(C1=O)[nH]c1c2ccc(c1)CC(=O)O\n",
      "O=c1n(/N=C/c2cc(O)c([N+](=O)[O-])cc2)cc(=O)cn1C\n",
      "O=C(NCc1ccncc1)/C=C/c1ccc2c(n1)[nH]cc2\n",
      "n1c(nc2c(c1N)ncn2[C@@H]1O[C@@H]([C@H]([C@@H]1O)O)CO)N\n",
      "N1(C(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@@H]1Cc1c[nH]c2c1cccc2)Cc1[nH]cnc1)Cc1ccccc1)CC(C)C)C(C)C)C\n",
      "O=S(=O)(c1ccc(cc1)NCl)NCCc1ccccc1\n",
      "O=S(=O)(n1c(=O)[nH]c(=O)cc1)N\n",
      "Brc1cc(C(=O)Nc2ccc(OC)cc2)ccc1\n",
      "ClC(Cl)C(Cl)(Cl)Cl\n",
      "C(=O)(c1c(c[nH]c1C1(O)CN(CC1)C)C)Nc1cc(ccc1)C\n",
      "c1c(ccc(c1)C(=O)/C=C\\1/C(=O)N(C1=O)c1ccccc1)C(=O)O\n",
      "c1(cc(c2c(c1)ncn2C1CCCCC1)n1cnc2c1ncn2C)Cl\n",
      "S(c1ccccc1)CC(C(=O)Nc1ccccc1)C\n",
      "S(c1n(c(nn1)c1ccncc1)C)CC(=O)Nc1ccc(cc1)C\n",
      "c1ccc2c(c1)cc(c(=O)o2)CSc1nc2c(n1)cccc2\n",
      "c1(c(cc(c(c1)C(=O)/C=C/c1ccc(cc1)OC)OC)OC)OC\n",
      "C1C(CCCC1)N[C@@H](C/C=C/c1n(ccc1C)C)O\n",
      "Cl.n1cc(c(c2ncc(cc12)c1cc[nH]n1)Cl)N[C@H]1CC[C@@H](CC1)NCCCCN\n",
      "s1c2nn(c3ncn4c(nc(c4c3occ4)C)c3c2c(OC)cccc3)c(=O)cc1\n",
      "[C@]12(C(=CC[C@H]3[C@]1(CC[C@@H]1[C@@]3(CC(CC1(C)C)C)C)CC2)C)[C@H]1[C@@](CC(=O)O[C@@H]1CCCC(=O)OC)(CC1)C\n",
      "O1/C(=C\\c2ccc(nc2)NC)/C(=O)N(C1=O)CCc1ccccc1\n",
      "OCCOc1ccc(cc1)c1ccc(cc1)C(CN1CCCCC1)C(=O)c1ccc(F)c(c1)C(F)(F)F\n",
      "Cl.c12c(O[C@@H](C(=O)NCCC(=O)O)C)cccc2N(c1cccc2)CCCCCCNCCCNCCNC(=O)Cc1occc1\n",
      "c1ccc2c(c1)cc(c(=O)[nH]2)C1=NCCN1\n",
      "C1CC(O1)(c1ccc(Nc2c(ccc(c2)Br)c2ccccc2)cc1Cl)C(=O)CCC=C(C)C\n",
      "COc1ccc(cc1)c1c(n2-c3c(C(=O)N)c(F)cc3cc(C2C)c2cnc(N)c(Cc3NCCNC1C)c2)c1cc(O)ccc1\n",
      "c1(cc(cc(c1)CCN(C)Cc1ccc(cc1)C)C)O\n",
      "c1cccc(c1)C(C(=O)N(C[C@H](NC(=O)c1cc(c(cc1)F)F)CC(=O)N(C)C)C)CCC\n",
      "O(C(=O)[C@]1(N(C(=O)N(CC1)Cc1ccccc1)CCc1ccc(cc1)C)Cc1ccccc1)C\n",
      "C1(=O)C(=C(C(=O)O1)c1ccccc1)C\n",
      "c1n(c(cc1)C[C@@H]1N([C@H](CN(C1)c1c(cccc1)F)CCCC)C)C1=CC(=O)NC(c2ccccc2)C(=O)[C@H](CC1)NC(=O)OC(C)(C)C\n",
      "Cl.Clc1c(C(=O)N[N+](=O)[O-])c(C(=O)N2CCOCC2)cc(c(Cl)c1)Cl\n",
      "c1ccc2c(c1)cc(c(=O)o2)Cc1ccccc1\n",
      "CC(=O)NCc1oc2c(n1)cccc2OCCc1cc2c(C)cccc2[nH]1\n",
      "c1cc(sc1c1ccc(cc1)CNCCn1cncc1)C\n",
      "c1(cc(cc(c1)CN1CCN(CC1)CC1CCCC1)OCC)[N+](=O)[O-]\n"
     ]
    }
   ],
   "source": [
    "smiles = model.generateSmiles(batch_size=100, vocab=vocab)\n",
    "\n",
    "for e in smiles:\n",
    "    smile = \"\".join(e)\n",
    "    print(smile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4691753212328171,\n",
       " 0.442591048512063,\n",
       " 0.4294968842674778,\n",
       " 0.4221685946537415,\n",
       " 0.4163481026534506,\n",
       " 0.41281202883117285,\n",
       " 0.41007964122593793,\n",
       " 0.4071080360330926,\n",
       " 0.40542209823769226,\n",
       " 0.4029807088324832,\n",
       " 0.40209448192273517,\n",
       " 0.4011382240316142,\n",
       " 0.39953339504316077,\n",
       " 0.398818506200323,\n",
       " 0.39824705318656994,\n",
       " 0.3973310890169483,\n",
       " 0.396317089656282,\n",
       " 0.3951411776548946,\n",
       " 0.3955495490622615,\n",
       " 0.39444996936833276,\n",
       " 0.3940663937680492,\n",
       " 0.3933146997638371,\n",
       " 0.3934417799684527,\n",
       " 0.3923208140651542,\n",
       " 0.3924812851216011,\n",
       " 0.39193774337668036,\n",
       " 0.3913349131507522,\n",
       " 0.391029222209463,\n",
       " 0.39056187844559137,\n",
       " 0.3905919809115263]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5854172040226433,\n",
       " 0.49285685749885355,\n",
       " 0.4729587226889993,\n",
       " 0.46224466937697867,\n",
       " 0.4550276200336772,\n",
       " 0.4497474878044135,\n",
       " 0.4456245037938862,\n",
       " 0.44239162675785876,\n",
       " 0.43975009340436816,\n",
       " 0.43765332727111633,\n",
       " 0.4357142901648482,\n",
       " 0.43413144653575864,\n",
       " 0.43265009342877586,\n",
       " 0.4314353059272508,\n",
       " 0.43034084716869486,\n",
       " 0.42929243282141777,\n",
       " 0.4283655682773911,\n",
       " 0.4274879257067616,\n",
       " 0.42676096733219937,\n",
       " 0.42603677773483506,\n",
       " 0.42543337051618546,\n",
       " 0.4247567388735559,\n",
       " 0.42419050116246676,\n",
       " 0.4236377225396817,\n",
       " 0.4231097109139398,\n",
       " 0.42267184156922033,\n",
       " 0.42219669078633276,\n",
       " 0.4217716067164999,\n",
       " 0.4213778995414588,\n",
       " 0.4209647733538891]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4177311360836029\n",
      "0.41916099190711975\n",
      "0.4311732351779938\n",
      "0.41949591040611267\n",
      "0.42207640409469604\n",
      "0.43419331312179565\n",
      "0.4199746549129486\n",
      "0.4349762499332428\n",
      "0.42442595958709717\n",
      "0.40924519300460815\n",
      "0.4224224388599396\n",
      "0.42739105224609375\n",
      "0.4201262593269348\n",
      "0.41302481293678284\n",
      "0.4397747814655304\n",
      "0.4295125901699066\n",
      "0.42633238434791565\n",
      "0.42718830704689026\n",
      "0.42273250222206116\n",
      "0.4282897710800171\n",
      "0.4235767424106598\n",
      "0.432208776473999\n",
      "0.4370172619819641\n",
      "0.4273785948753357\n",
      "0.4169257879257202\n",
      "0.4105018973350525\n",
      "0.4319404065608978\n",
      "0.4151376187801361\n",
      "0.43899813294410706\n",
      "0.4210602343082428\n",
      "Epoch: 1 train loss: 0.42543337051618546 val loss: 0.3940663937680492\n",
      "0.41826513409614563\n",
      "0.4200685918331146\n",
      "0.43118324875831604\n",
      "0.41723471879959106\n",
      "0.4194599986076355\n",
      "0.42983534932136536\n",
      "0.4214036762714386\n",
      "0.4338279962539673\n",
      "0.4267996549606323\n",
      "0.4090276062488556\n",
      "0.4208979308605194\n",
      "0.42840349674224854\n",
      "0.41869232058525085\n",
      "0.4145174026489258\n",
      "0.4396442472934723\n",
      "0.43003275990486145\n",
      "0.42692843079566956\n",
      "0.4248432219028473\n",
      "0.4226573705673218\n",
      "0.4300452470779419\n",
      "0.42394495010375977\n",
      "0.43156325817108154\n",
      "0.433566153049469\n",
      "0.4274497330188751\n",
      "0.4167737364768982\n",
      "0.4107617735862732\n",
      "0.4314368963241577\n",
      "0.414546936750412\n",
      "0.4379226565361023\n",
      "0.41963183879852295\n",
      "Epoch: 2 train loss: 0.4247567388735559 val loss: 0.3933146997638371\n",
      "0.41490280628204346\n",
      "0.4165477454662323\n",
      "0.43195194005966187\n",
      "0.4176918864250183\n",
      "0.4209003746509552\n",
      "0.42867255210876465\n",
      "0.4187472462654114\n",
      "0.4331514239311218\n",
      "0.42459291219711304\n",
      "0.41164910793304443\n",
      "0.4204593598842621\n",
      "0.42611953616142273\n",
      "0.4178054928779602\n",
      "0.4137621819972992\n",
      "0.43862035870552063\n",
      "0.42869383096694946\n",
      "0.4253087639808655\n",
      "0.4236621558666229\n",
      "0.42193546891212463\n",
      "0.4260944724082947\n",
      "0.4222670793533325\n",
      "0.43088647723197937\n",
      "0.4355418384075165\n",
      "0.42517945170402527\n",
      "0.4154610335826874\n",
      "0.4095015823841095\n",
      "0.4285501539707184\n",
      "0.4163956344127655\n",
      "0.43841537833213806\n",
      "0.4222465753555298\n",
      "Epoch: 3 train loss: 0.42419050116246676 val loss: 0.3934417799684527\n",
      "0.4164056181907654\n",
      "0.4192489981651306\n",
      "0.43083953857421875\n",
      "0.41758134961128235\n",
      "0.4217403829097748\n",
      "0.4275319278240204\n",
      "0.4215122163295746\n",
      "0.43434882164001465\n",
      "0.4243603050708771\n",
      "0.40927791595458984\n",
      "0.4210914075374603\n",
      "0.4263661205768585\n",
      "0.4186933636665344\n",
      "0.41172122955322266\n",
      "0.43786606192588806\n",
      "0.42914050817489624\n",
      "0.4256838262081146\n",
      "0.4250907599925995\n",
      "0.4195883870124817\n",
      "0.42706453800201416\n",
      "0.42097458243370056\n",
      "0.4286694824695587\n",
      "0.4332444667816162\n",
      "0.426378071308136\n",
      "0.415569931268692\n",
      "0.408947616815567\n",
      "0.42890557646751404\n",
      "0.4135981798171997\n",
      "0.4366605281829834\n",
      "0.42017412185668945\n",
      "Epoch: 4 train loss: 0.4236377225396817 val loss: 0.3923208140651542\n",
      "0.4159623086452484\n",
      "0.4162493348121643\n",
      "0.4320303797721863\n",
      "0.4164651036262512\n",
      "0.4207918643951416\n",
      "0.4272419810295105\n",
      "0.41863423585891724\n",
      "0.43452945351600647\n",
      "0.4230472147464752\n",
      "0.40947234630584717\n",
      "0.421078622341156\n",
      "0.4276515245437622\n",
      "0.41601264476776123\n",
      "0.4136499762535095\n",
      "0.43656548857688904\n",
      "0.42799827456474304\n",
      "0.42632022500038147\n",
      "0.425364226102829\n",
      "0.42131516337394714\n",
      "0.4125863313674927\n",
      "0.41626209020614624\n",
      "0.42838457226753235\n",
      "0.4149857461452484\n",
      "0.4168473780155182\n",
      "0.42724281549453735\n",
      "0.4164007604122162\n",
      "0.43226462602615356\n",
      "0.42282724380493164\n",
      "0.40842345356941223\n",
      "0.41995906829833984\n",
      "0.42413330078125\n",
      "0.41678792238235474\n",
      "0.41113558411598206\n",
      "0.43515026569366455\n",
      "0.4277568757534027\n",
      "0.42673036456108093\n",
      "0.4223562777042389\n",
      "0.4176764488220215\n",
      "0.4251837432384491\n",
      "0.42034927010536194\n",
      "0.42644962668418884\n",
      "0.43408727645874023\n",
      "0.42260095477104187\n",
      "0.4150111973285675\n",
      "0.4076300859451294\n",
      "0.42613717913627625\n",
      "0.40895816683769226\n",
      "0.4327227771282196\n",
      "0.416960209608078\n",
      "Epoch: 8 train loss: 0.4217716067164999 val loss: 0.391029222209463\n",
      "0.4134253263473511\n",
      "0.4148537516593933\n",
      "0.4294852912425995\n",
      "0.41373658180236816\n",
      "0.4167879521846771\n",
      "0.42433708906173706\n",
      "0.416698694229126\n",
      "0.4311474859714508\n",
      "0.42148351669311523\n",
      "0.40876755118370056\n",
      "0.4192578196525574\n",
      "0.4233340620994568\n",
      "0.41648390889167786\n",
      "0.4104035794734955\n",
      "0.4350214898586273\n",
      "0.4257924556732178\n",
      "0.4242824614048004\n",
      "0.4222879409790039\n",
      "0.4181218147277832\n",
      "0.4254748821258545\n",
      "0.41700655221939087\n",
      "0.42789918184280396\n",
      "0.4318143427371979\n",
      "0.4231436848640442\n",
      "0.4134669005870819\n",
      "0.40765270590782166\n",
      "0.4257703125476837\n",
      "0.4111495614051819\n",
      "0.4367937743663788\n",
      "0.41578906774520874\n",
      "Epoch: 9 train loss: 0.4213778995414588 val loss: 0.39056187844559137\n",
      "0.41271501779556274\n",
      "0.41596829891204834\n",
      "0.4276329576969147\n",
      "0.41298598051071167\n",
      "0.4169590175151825\n",
      "0.42632368206977844\n",
      "0.4152284562587738\n",
      "0.4296335279941559\n",
      "0.4205775260925293\n",
      "0.40542349219322205\n",
      "0.41757890582084656\n",
      "0.4247482120990753\n",
      "0.41575106978416443\n",
      "0.4112750291824341\n",
      "0.43482959270477295\n",
      "0.42439860105514526\n",
      "0.4218781888484955\n",
      "0.4209681749343872\n",
      "0.41750186681747437\n",
      "0.4233669340610504\n",
      "0.4196089208126068\n",
      "0.4259081482887268\n",
      "0.43199408054351807\n",
      "0.4233502447605133\n",
      "0.4134480357170105\n",
      "0.4074726402759552\n",
      "0.42626962065696716\n",
      "0.4090728461742401\n",
      "0.43658989667892456\n",
      "0.4158417582511902\n",
      "Epoch: 10 train loss: 0.4209647733538891 val loss: 0.3905919809115263\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    tl = train(model, optimizer, criterion, train_loader)\n",
    "    vl = valid(model, criterion, val_loader)\n",
    "\n",
    "    train_result.append(tl)\n",
    "    val_result.append(vl)\n",
    "\n",
    "    print(\"Epoch:\", i+1, \"train loss:\", tl, \"val loss:\", vl)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    count = 200\n",
    "    for e in train_loader:\n",
    "        x = e['idx'].to(DEVICE)\n",
    "        pad_mask = e['pad_mask'].to(DEVICE)\n",
    "\n",
    "        x_input = x[:, :-1]\n",
    "        pad_mask = pad_mask[:, :-1]\n",
    "        y_expected = x[:, 1:]\n",
    "        output = model(x_input, pad_mask)\n",
    "        #print(output.shape, y_expected.shape)\n",
    "\n",
    "        output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "        y_expected = torch.flatten(y_expected, start_dim=0, end_dim=1)\n",
    "        loss = criterion(output, y_expected)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        if (count+1)%100 == 0: print(loss.item())\n",
    "        \n",
    "    total_loss = total_loss/len(train_loader)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# valid function\n",
    "def valid(model, criterion, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for e in val_loader:\n",
    "            x = e['idx'].to(DEVICE)\n",
    "            pad_mask = e['pad_mask'].to(DEVICE)\n",
    "\n",
    "            x_input = x[:, :-1]\n",
    "            pad_mask = pad_mask[:, :-1]\n",
    "            y_expected = x[:, 1:]\n",
    "\n",
    "            output = model(x_input, pad_mask)\n",
    "            \n",
    "            output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "            y_expected = torch.flatten(y_expected, start_dim=0, end_dim=1)\n",
    "            loss = criterion(output, y_expected)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    total_loss = total_loss/len(val_loader)\n",
    "   \n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(10, 10)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (multihead): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=10, out_features=10, bias=True)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Decoder(10, 10, 1, 10)\n",
    "\n",
    "m.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['=', 'C', '/'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " [],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['\\\\'],\n",
       " ['invalid'],\n",
       " ['=', '\\\\'],\n",
       " ['invalid'],\n",
       " ['/', 'C', '/', 'O'],\n",
       " [],\n",
       " ['\\\\'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['/'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['\\\\', '/'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " ['invalid'],\n",
       " [],\n",
       " ['invalid'],\n",
       " ['invalid']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generateSmiles(100, getVocab(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(10, 10)\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (multihead): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=10, out_features=10, bias=True)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(next(m.parameters()).device.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(m.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4, 4],\n",
       "        [4, 4, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(\n",
    "        torch.full((4, 4), float('-inf'), dtype=torch.float),\n",
    "        diagonal=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(4, 4) == 1)\n",
    "mask = mask.float()\n",
    "mask = mask.masked_fill(mask == 0, float('-inf')) \n",
    "mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [ True,  True, False, False],\n",
       "        [ True,  True,  True, False],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(4, 4) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(4, 4) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       "  (multihead_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=10, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.TransformerDecoderLayer(d_model=10, nhead=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
